{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot_demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiayiwang5/Google-Colab/blob/master/chatbot_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "BPOsyEiC1MVI",
        "colab_type": "code",
        "outputId": "79e66e35-45d1-4c1f-a514-7d244fc692f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import operator\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "data_dir = np.arange(0, 3000, 1000)\n",
        "main_path = \"/content/drive/My Drive/Colab Notebooks/\"\n",
        "def get_file_list(file_path):\n",
        "    dir_list = os.listdir(file_path)\n",
        "    if not dir_list:\n",
        "        return\n",
        "    else:\n",
        "        dir_list = sorted(dir_list, key=lambda x: os.path.getmtime(os.path.join(file_path, x)))\n",
        "    return dir_list\n",
        "with open(main_path + 'middle_data/dictionary.pkl', 'rb') as f:\n",
        "    word_to_index = pickle.load(f)\n",
        "for i, j in word_to_index.items():\n",
        "      word_to_index[i] = j + 1\n",
        "index_to_word = {}\n",
        "for key, value in word_to_index.items():\n",
        "    index_to_word[value] = key\n",
        "with open(main_path + 'middle_data/words.pkl', 'rb') as f:\n",
        "    words = pickle.load(f)\n",
        "maxLen = 20\n",
        "vocab_size = len(word_to_index) + 1\n",
        "print('word_to_vec_map: ', len(list(words)))\n",
        "print('vocab_size: ', vocab_size)\n",
        "with open(main_path + 'middle_data/embedding_matrix.pkl', 'rb') as f:\n",
        "      embedding_matrix = pickle.load(f)\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word_to_vec_map:  400000\n",
            "vocab_size:  42905\n",
            "(42905, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GqcLHzpro5Tb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2437c0e8-7ea6-433a-d44f-cf70218d372b"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence\n",
        "def generate_train(batch_size):\n",
        "    print('\\n*********************************generate_train()*********************************')\n",
        "    steps=0\n",
        "    context_ = np.load(main_path + 'middle_data/context_indexes.npy')\n",
        "    final_target_ = np.load(main_path + 'middle_data/target_indexes.npy')\n",
        "    context_ = context_[:90000]\n",
        "    final_target_ = final_target_[:90000]\n",
        "    epoch_mark = 0\n",
        "    while True:\n",
        "        context = context_[steps:steps+batch_size]\n",
        "        final_target = final_target_[steps:steps+batch_size]\n",
        "        if epoch_mark == 0:\n",
        "          for pos, i in enumerate(final_target):\n",
        "              for pos_, j in enumerate(i):\n",
        "                  i[pos_] = j + 1\n",
        "              if(len(i) > maxLen):\n",
        "                  final_target[pos] = i[:maxLen]\n",
        "\n",
        "          for pos, i in enumerate(context):\n",
        "              for pos_, j in enumerate(i):\n",
        "                  i[pos_] = j + 1\n",
        "              if(len(i) > maxLen):\n",
        "                  context[pos] = i[:maxLen]\n",
        "  #         print('\\ncontext.shape: ', context.shape)\n",
        "          outs = np.zeros([context.shape[0], maxLen, vocab_size], dtype='float32')\n",
        "          for pos, i in enumerate(final_target):\n",
        "              for pos_, j in enumerate(i):\n",
        "                  if pos_ > 20:\n",
        "                      print(i)\n",
        "                  if pos_ > 0:\n",
        "                      outs[pos, pos_-1, j] = 1 # one-hot\n",
        "  #             if pos%1000 == 0 :\n",
        "  #                 print('{} entries completed'.format(pos)) # format()填充{}，格式化输出\n",
        "  #         print('\\nouts.shape: ', outs.shape)\n",
        "        final_target = sequence.pad_sequences(final_target, maxlen=maxLen,\n",
        "                                        dtype='int32', padding='post', \n",
        "                                         truncating='post')\n",
        "        context = sequence.pad_sequences(context, maxlen=maxLen,\n",
        "                                   dtype='int32', padding='post',\n",
        "                                   truncating='post')\n",
        "        yield ([context, final_target], outs)\n",
        "        steps += batch_size\n",
        "        if steps == 90000:\n",
        "            steps = 0\n",
        "            epoch_mark += 1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "K4gl3z7Tyczw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence\n",
        "def generate_test(batch_size):\n",
        "    print('\\n*********************************generate_test()*********************************')\n",
        "    steps=0\n",
        "    context_ = np.load(main_path + 'middle_data/context_indexes.npy')\n",
        "    final_target_ = np.load(main_path + 'middle_data/target_indexes.npy')\n",
        "    context_ = context_[90000:]\n",
        "    final_target_ = final_target_[90000:]\n",
        "    epoch_mark = 0\n",
        "    while True:\n",
        "        context = context_[steps:steps+batch_size]\n",
        "        final_target = final_target_[steps:steps+batch_size]\n",
        "        if epoch_mark == 0:\n",
        "          for pos, i in enumerate(final_target):\n",
        "              for pos_, j in enumerate(i):\n",
        "                  i[pos_] = j + 1\n",
        "              if(len(i) > maxLen):\n",
        "                  final_target[pos] = i[:maxLen]\n",
        "\n",
        "          for pos, i in enumerate(context):\n",
        "              for pos_, j in enumerate(i):\n",
        "                  i[pos_] = j + 1\n",
        "              if(len(i) > maxLen):\n",
        "                  context[pos] = i[:maxLen]\n",
        "  #         print('\\ncontext.shape: ', context.shape)\n",
        "          outs = np.zeros([context.shape[0], maxLen, vocab_size], dtype='float32')\n",
        "          for pos, i in enumerate(final_target):\n",
        "              for pos_, j in enumerate(i):\n",
        "                  if pos_ > 20:\n",
        "                      print(i)\n",
        "                  if pos_ > 0:\n",
        "                      outs[pos, pos_-1, j] = 1 # one-hot\n",
        "  #             if pos%1000 == 0 :\n",
        "  #                 print('{} entries completed'.format(pos)) # format()填充{}，格式化输出\n",
        "  #         print('\\nouts.shape: ', outs.shape)\n",
        "        final_target = sequence.pad_sequences(final_target, maxlen=maxLen,\n",
        "                                        dtype='int32', padding='post', \n",
        "                                         truncating='post')\n",
        "        context = sequence.pad_sequences(context, maxlen=maxLen,\n",
        "                                   dtype='int32', padding='post',\n",
        "                                   truncating='post')\n",
        "        yield ([context, final_target], outs)\n",
        "        steps += batch_size\n",
        "        if steps == 10000:\n",
        "            steps = 0\n",
        "            epoch_mark += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KyXaCfyZo-ch",
        "colab_type": "code",
        "outputId": "efc26aca-d6e4-4b9f-9da4-e91263b49097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding\n",
        "from keras.layers import Input, Dense, LSTM, TimeDistributed, Bidirectional, Concatenate\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "embed_layer = Embedding(input_dim=vocab_size, output_dim=50, trainable=True)\n",
        "embed_layer.build((None,))\n",
        "embed_layer.set_weights([embedding_matrix])\n",
        "\n",
        "LSTM_cell = Bidirectional(LSTM(512, return_sequences=True, return_state=True))\n",
        "LSTM_decoder = LSTM(1024, return_sequences=True, return_state=True)\n",
        "LSTM_propagate = LSTM(1024, return_state=True)\n",
        "\n",
        "dense = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "  #encoder输入 与 decoder输入\n",
        "input_context = Input(shape=(None, ), dtype='int32', name='input_context')\n",
        "input_target = Input(shape=(None, ), dtype='int32', name='input_target')\n",
        "\n",
        "input_context_embed = embed_layer(input_context)\n",
        "input_target_embed = embed_layer(input_target)\n",
        "\n",
        "encoder_out, forward_h, forward_c, backward_h, backward_c = LSTM_cell(input_context_embed)\n",
        "context_h = Concatenate()([forward_h, backward_h])\n",
        "context_c = Concatenate()([forward_c, backward_c])\n",
        "\n",
        "encoder_pro, pro_h, pro_c = LSTM_propagate(encoder_out, initial_state=[context_h, context_c])\n",
        "\n",
        "decoder_lstm, _, _ = LSTM_decoder(input_target_embed, \n",
        "                                    initial_state=[pro_h, pro_c])\n",
        "\n",
        "output = dense(decoder_lstm)\n",
        "\n",
        "model = Model([input_context, input_target], output)\n",
        "\n",
        "rmsprop = RMSprop(lr=0.005, rho=0.9, epsilon=None, decay=0.0)\n",
        "\n",
        "model.compile(optimizer=rmsprop, loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "#   model.summary() \n",
        "\n",
        "filepath = main_path + \"modles/W2-\" + \"-{epoch:03d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath,\n",
        "                             monitor='loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=False,\n",
        "                             mode='min',\n",
        "                             period=1,\n",
        "                             save_weights_only=True\n",
        "                             )\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', \n",
        "                              factor=0.2, \n",
        "                              patience=5, \n",
        "                              verbose=1, \n",
        "                              mode='min', \n",
        "                              min_delta=0.0001, \n",
        "                              cooldown=0, \n",
        "                              min_lr=0\n",
        "                              )\n",
        "callbacks_list = [checkpoint, reduce_lr]\n",
        "  \n",
        "initial_epoch=0\n",
        "file_list = os.listdir(main_path + 'modles/')\n",
        "if len(file_list) > 0:\n",
        "  epoch_list = get_file_list(main_path + 'modles/')\n",
        "  epoch_last = epoch_list[-1]\n",
        "  model.load_weights(main_path + 'modles/' + epoch_last)\n",
        "#     if len(file_list) > 2:\n",
        "#         for file_name in file_list[:-2]:\n",
        "#             file_ = main_path + 'modles/' + file_name\n",
        "#             os.remove(file_)\n",
        "#             print('Removed Successful! -- ', file_name)\n",
        "  print(\"**********checkpoint_loaded: \", epoch_last)\n",
        "  initial_epoch = int(epoch_last.split('-')[2]) - 1\n",
        "  print('**********Begin from epoch: ', str(initial_epoch))\n",
        "    \n",
        "\n",
        "model.fit_generator(generate_train(batch_size=100), \n",
        "                    steps_per_epoch=900, # (total samples) / batch_size 90000/100 = 900\n",
        "                    epochs=100, \n",
        "                    verbose=1, \n",
        "                    callbacks=callbacks_list, \n",
        "                    validation_data=generate_test(batch_size=100), \n",
        "                    validation_steps=100, # 10000/100 = 100\n",
        "                    class_weight=None, \n",
        "                    max_queue_size=10, \n",
        "                    workers=1, \n",
        "                    use_multiprocessing=False, \n",
        "                    shuffle=False, \n",
        "                    initial_epoch=initial_epoch\n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/100\n",
            "\n",
            "*********************************generate_test()*********************************\n",
            "*********************************generate_train()*********************************\n",
            "\n",
            "900/900 [==============================] - 889s 988ms/step - loss: 1.9234 - acc: 0.1423 - val_loss: 1.8620 - val_acc: 0.1533\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/modles/W2--001-1.9234-bigger.hdf5\n",
            "Epoch 2/100\n",
            "658/900 [====================>.........] - ETA: 3:28 - loss: 1.4919 - acc: 0.0689"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gmiOjkRzLGI_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x95onNF_JGKf",
        "colab_type": "code",
        "outputId": "649f085f-94ea-4c3f-b90a-290f56dce32f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks/modles/\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "W2--001-2.0162-bigger.hdf5  W2--007-1.9841-bigger.hdf5\n",
            "W2--002-2.0124-bigger.hdf5  W2--008-1.9831-bigger.hdf5\n",
            "W2--003-1.9936-bigger.hdf5  W2--009-2.1618-bigger.hdf5\n",
            "W2--004-1.9889-bigger.hdf5  W2--010-2.1511-bigger.hdf5\n",
            "W2--005-1.9830-bigger.hdf5  W2--011-2.1385-bigger.hdf5\n",
            "W2--006-1.9849-bigger.hdf5  W2--012-2.4226-bigger.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}