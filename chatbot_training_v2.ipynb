{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot_training_v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiayiwang5/Google-Colab/blob/master/chatbot_training_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Q44JserZZE7w",
        "colab_type": "code",
        "outputId": "07ed08c0-3b29-480f-91d8-fe7ef6cb1e08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints\n",
        "\n",
        "\n",
        "class AttentionL(Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(AttentionL, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
        "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0],  self.features_dim\n",
        "\n",
        "    def get_config(self):\n",
        "        config={'step_dim':self.step_dim}\n",
        "        base_config = super(AttentionL, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "BPOsyEiC1MVI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import operator\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "# # This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "# TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "# tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "# data_dir = np.arange(0, 105000, 5000)\n",
        "data_dir = np.arange(0, 101000, 1000)\n",
        "last_model_index = 0\n",
        "main_path = '/content/drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "def get_file_list(file_path):\n",
        "    dir_list = os.listdir(file_path)\n",
        "    if not dir_list:\n",
        "        return\n",
        "    else:\n",
        "        # os.path.getmtime() 函数是获取文件最后修改时间\n",
        "        # os.path.getctime() 函数是获取文件最后创建时间\n",
        "        dir_list = sorted(dir_list, key=lambda x: os.path.getmtime(os.path.join(file_path, x)))\n",
        "        # print(dir_list)\n",
        "    return dir_list\n",
        "\n",
        "if len(os.listdir(main_path + 'modles/')) > 0:\n",
        "  last_model_list = get_file_list(main_path + 'modles/')\n",
        "  last_model_name = str(last_model_list[-1])\n",
        "  last_model_number = last_model_name.split('-')[1]\n",
        "  last_model_index = np.where(data_dir == int(last_model_number))[0][0]\n",
        "  print(last_model_number, last_model_index)\n",
        "\n",
        "  \n",
        "for A_pos, A_name in enumerate(data_dir[last_model_index:]):\n",
        "  data_path = main_path + 'data_sections_small/'\n",
        "  A_name = str(A_name)\n",
        "  \n",
        "  print('Begin Loading from File... '+ data_path + A_name)\n",
        "  context = np.load(data_path + A_name + '/context_indexes.npy')\n",
        "  final_target = np.load(data_path + A_name + '/target_indexes.npy')\n",
        "  \n",
        "  with open(main_path + 'middle_data/dictionary.pkl', 'rb') as f:\n",
        "      word_to_index = pickle.load(f)\n",
        "\n",
        "\n",
        "  '''\n",
        "      the indexes of the words start with 0. \n",
        "      But when the sequences are padded later on, they too will be zeros.\n",
        "      so, shift all the index values one position to the right, \n",
        "      so that 0 is spared, and used only to pad the sequences\n",
        "  '''\n",
        "  for i, j in word_to_index.items():\n",
        "      word_to_index[i] = j + 1\n",
        "\n",
        "  index_to_word = {}\n",
        "  for key, value in word_to_index.items():\n",
        "      index_to_word[value] = key\n",
        "\n",
        "  final_target_ = final_target\n",
        "  context_ = context\n",
        "  maxLen = 20\n",
        "\n",
        "  for pos, i in enumerate(final_target_):\n",
        "      for pos_, j in enumerate(i):\n",
        "          i[pos_] = j + 1\n",
        "      if(len(i) > maxLen):\n",
        "          final_target_[pos] = i[:maxLen]\n",
        "\n",
        "  for pos, i in enumerate(context_):\n",
        "      for pos_, j in enumerate(i):\n",
        "          i[pos_] = j + 1\n",
        "      if(len(i) > maxLen):\n",
        "          context_[pos] = i[:maxLen]\n",
        "  print(context_.shape)\n",
        "  # print(context_)\n",
        "\n",
        "\n",
        "  with open(main_path + 'middle_data/words.pkl', 'rb') as f:\n",
        "      words = pickle.load(f)\n",
        "\n",
        "  '''\n",
        "  since the indexes start from 1 and not 0, \n",
        "  we add 1 to the no. of total words to get the vocabulary size \n",
        "  (while initializing and populating arrays later on, this will be required)\n",
        "  '''\n",
        "  vocab_size = len(word_to_index) + 1\n",
        "  print('word_to_vec_map: ', len(list(words)))\n",
        "  print('vocab_size: ', vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "  with open(main_path + 'middle_data/embedding_matrix.pkl', 'rb') as f:\n",
        "      embedding_matrix = pickle.load(f)\n",
        "\n",
        "  print(embedding_matrix.shape)\n",
        "\n",
        "  # outs为final_target_左一位偏移\n",
        "  # (样本数，最大句子长度，词表大小)\n",
        "  outs = np.zeros([context_.shape[0], maxLen, vocab_size], dtype='float32')\n",
        "  for pos, i in enumerate(final_target_):\n",
        "      for pos_, j in enumerate(i):\n",
        "          if pos_ > 20:\n",
        "              print(i)\n",
        "          if pos_ > 0:\n",
        "              outs[pos, pos_-1, j] = 1 # one-hot\n",
        "      if pos%1000 == 0 :\n",
        "          print('{} entries completed'.format(pos)) # format()填充{}，格式化输出\n",
        "  print(outs.shape)\n",
        "  # print(outs[0])\n",
        "\n",
        "  from keras.preprocessing import sequence\n",
        "  #后端padding\n",
        "  final_target_ = sequence.pad_sequences(final_target_, maxlen=maxLen,\n",
        "                                        dtype='int32', padding='post', \n",
        "                                         truncating='post')\n",
        "  context_ = sequence.pad_sequences(context_, maxlen=maxLen,\n",
        "                                   dtype='int32', padding='post',\n",
        "                                   truncating='post')\n",
        "  # print(context_)\n",
        "\n",
        "  from keras.layers import Embedding\n",
        "  from keras.layers import Input, Dense, LSTM, TimeDistributed, Bidirectional, Concatenate, Dropout, Activation, Dot, RepeatVector\n",
        "  from keras.models import Model\n",
        "  from keras.utils import plot_model\n",
        "  from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "  embed_layer = Embedding(input_dim=vocab_size, output_dim=50, trainable=True)\n",
        "  embed_layer.build((None,))\n",
        "  embed_layer.set_weights([embedding_matrix])\n",
        "\n",
        "  LSTM_cell = Bidirectional(LSTM(512, return_sequences=True, return_state=True))\n",
        "  LSTM_decoder = LSTM(1024, return_sequences=True, return_state=True)\n",
        "\n",
        "  dense = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "  #encoder输入 与 decoder输入\n",
        "  input_context = Input(shape=(maxLen, ), dtype='int32', name='input_context')\n",
        "  input_target = Input(shape=(maxLen, ), dtype='int32', name='input_target')\n",
        "\n",
        "  input_context_embed = embed_layer(input_context)\n",
        "  input_target_embed = embed_layer(input_target)\n",
        "\n",
        "  encoder_out, forward_h, forward_c, backward_h, backward_c = LSTM_cell(input_context_embed)\n",
        "  context_h = Concatenate()([forward_h, backward_h])\n",
        "  context_c = Concatenate()([forward_c, backward_c])\n",
        "  decoder_lstm, _, _ = LSTM_decoder(input_target_embed, \n",
        "                                    initial_state=[context_h, context_c])\n",
        "  \n",
        "  concatenator = Concatenate(axis=-1)\n",
        "  attention = AttentionL(maxLen)(encoder_out)\n",
        "  attention = RepeatVector(maxLen)(attention) \n",
        "  merge = concatenator([attention,decoder_lstm]) \n",
        "\n",
        "  output = dense(merge)\n",
        "\n",
        "  model = Model([input_context, input_target], output)\n",
        "\n",
        "  model.compile(optimizer='rmsprop', loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "  model.summary() \n",
        "\n",
        "  filepath = main_path + \"modles/weights-\" + A_name + \"-{epoch:03d}-{loss:.4f}-bigger.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath,\n",
        "                                 monitor='loss',\n",
        "                                 verbose=1,\n",
        "                                 save_best_only=True,\n",
        "                                 mode='min',\n",
        "                                 period=15,\n",
        "                                 save_weights_only=True)\n",
        "  callbacks_list = [checkpoint]\n",
        "  \n",
        "  initial_epoch=0\n",
        "  file_list = os.listdir(main_path + 'modles/')\n",
        "  if len(file_list) > 0:\n",
        "    epoch_list = get_file_list(main_path + 'modles/')\n",
        "    epoch_last = epoch_list[-1]\n",
        "    model.load_weights(main_path + 'modles/' + epoch_last)\n",
        "    if len(file_list) > 2:\n",
        "        for file_name in file_list[:-2]:\n",
        "            file_ = main_path + 'modles/' + file_name\n",
        "            os.remove(file_)\n",
        "            print('Removed Successful! -- ', file_name)\n",
        "    print(\"checkpoint_loaded: \", epoch_last)\n",
        "    if epoch_last.split('-')[2] == '015' and epoch_last.split('-')[1] == A_name:\n",
        "      initial_epoch = 15\n",
        "    if epoch_last.split('-')[2] == '030' and epoch_last.split('-')[1] == A_name:\n",
        "      initial_epoch = 30\n",
        "    print('Begin from epoch: ', str(initial_epoch))\n",
        "    \n",
        "    \n",
        "#     tpu_model = tf.contrib.tpu.keras_to_tpu_model(model,\n",
        "#                                                   strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "#                                                   tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "#     tpu_model.fit([context_, final_target_], \n",
        "#                 outs, \n",
        "#                 epochs=30, \n",
        "#                 batch_size=1*8, \n",
        "#                 validation_split=0.1, \n",
        "#                 callbacks=callbacks_list,\n",
        "#                 initial_epoch=initial_epoch\n",
        "#                 )\n",
        "\n",
        "  model.fit([context_, final_target_], \n",
        "            outs, \n",
        "            epochs=30, \n",
        "            batch_size=10, \n",
        "            validation_split=0.1, \n",
        "            callbacks=callbacks_list,\n",
        "            initial_epoch=initial_epoch,\n",
        "            verbose=2\n",
        "           )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gmiOjkRzLGI_",
        "colab_type": "code",
        "outputId": "4b0855ff-8478-4ad1-d669-1f8e4a756812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 59
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x95onNF_JGKf",
        "colab_type": "code",
        "outputId": "e93a09ad-566f-4805-ea75-c9e7001b283f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks/modles/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}