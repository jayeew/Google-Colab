{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot_training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiayiwang5/Google-Colab/blob/master/chatbot_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "BPOsyEiC1MVI",
        "colab_type": "code",
        "outputId": "8f10ff9f-79b7-4dcf-c140-ea28021a5718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2725
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import operator\n",
        "import os\n",
        "\n",
        "# data_dir = np.arange(0, 105000, 5000)\n",
        "data_dir = np.arange(0, 101000, 1000)\n",
        "\n",
        "main_path = '/content/drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "def get_file_list(file_path):\n",
        "    dir_list = os.listdir(file_path)\n",
        "    if not dir_list:\n",
        "        return\n",
        "    else:\n",
        "        # os.path.getmtime() 函数是获取文件最后修改时间\n",
        "        # os.path.getctime() 函数是获取文件最后创建时间\n",
        "        dir_list = sorted(dir_list, key=lambda x: os.path.getmtime(os.path.join(file_path, x)))\n",
        "        # print(dir_list)\n",
        "    return dir_list\n",
        "\n",
        "for A_pos, A_name in enumerate(data_dir):\n",
        "  data_path = main_path + 'data_sections_small/'\n",
        "  A_name = str(A_name)\n",
        "  \n",
        "  context = np.load(data_path + A_name + '/context_indexes.npy')\n",
        "  final_target = np.load(data_path + A_name + '/target_indexes.npy')\n",
        "  \n",
        "  with open(main_path + 'middle_data/dictionary.pkl', 'rb') as f:\n",
        "      word_to_index = pickle.load(f)\n",
        "\n",
        "\n",
        "  '''\n",
        "      the indexes of the words start with 0. \n",
        "      But when the sequences are padded later on, they too will be zeros.\n",
        "      so, shift all the index values one position to the right, \n",
        "      so that 0 is spared, and used only to pad the sequences\n",
        "  '''\n",
        "  for i, j in word_to_index.items():\n",
        "      word_to_index[i] = j + 1\n",
        "\n",
        "  index_to_word = {}\n",
        "  for key, value in word_to_index.items():\n",
        "      index_to_word[value] = key\n",
        "\n",
        "  final_target_ = final_target\n",
        "  context_ = context\n",
        "  maxLen = 20\n",
        "\n",
        "  for pos, i in enumerate(final_target_):\n",
        "      for pos_, j in enumerate(i):\n",
        "          i[pos_] = j + 1\n",
        "      if(len(i) > maxLen):\n",
        "          final_target_[pos] = i[:maxLen]\n",
        "\n",
        "  for pos, i in enumerate(context_):\n",
        "      for pos_, j in enumerate(i):\n",
        "          i[pos_] = j + 1\n",
        "      if(len(i) > maxLen):\n",
        "          context_[pos] = i[:maxLen]\n",
        "  print(context_.shape)\n",
        "  # print(context_)\n",
        "\n",
        "\n",
        "  with open(main_path + 'middle_data/words.pkl', 'rb') as f:\n",
        "      words = pickle.load(f)\n",
        "\n",
        "  '''\n",
        "  since the indexes start from 1 and not 0, \n",
        "  we add 1 to the no. of total words to get the vocabulary size \n",
        "  (while initializing and populating arrays later on, this will be required)\n",
        "  '''\n",
        "  vocab_size = len(word_to_index) + 1\n",
        "  print('word_to_vec_map: ', len(list(words)))\n",
        "  print('vocab_size: ', vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "  with open(main_path + 'middle_data/embedding_matrix.pkl', 'rb') as f:\n",
        "      embedding_matrix = pickle.load(f)\n",
        "\n",
        "  print(embedding_matrix.shape)\n",
        "\n",
        "  # outs为final_target_左一位偏移\n",
        "  # (样本数，最大句子长度，词表大小)\n",
        "  outs = np.zeros([context_.shape[0], maxLen, vocab_size], dtype='float32')\n",
        "  for pos, i in enumerate(final_target_):\n",
        "      for pos_, j in enumerate(i):\n",
        "          if pos_ > 20:\n",
        "              print(i)\n",
        "          if pos_ > 0:\n",
        "              outs[pos, pos_-1, j] = 1 # one-hot\n",
        "      if pos%1000 == 0 :\n",
        "          print('{} entries completed'.format(pos)) # format()填充{}，格式化输出\n",
        "  print(outs.shape)\n",
        "  # print(outs[0])\n",
        "\n",
        "  from keras.preprocessing import sequence\n",
        "  #后端padding\n",
        "  final_target_ = sequence.pad_sequences(final_target_, maxlen=maxLen,\n",
        "                                        dtype='int32', padding='post', \n",
        "                                         truncating='post')\n",
        "  context_ = sequence.pad_sequences(context_, maxlen=maxLen,\n",
        "                                   dtype='int32', padding='post',\n",
        "                                   truncating='post')\n",
        "  # print(context_)\n",
        "\n",
        "  from keras.layers import Embedding\n",
        "  from keras.layers import Input, Dense, LSTM, TimeDistributed\n",
        "  from keras.models import Model\n",
        "  from keras.utils import plot_model\n",
        "  from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "  embed_layer = Embedding(input_dim=vocab_size, output_dim=50, trainable=True)\n",
        "  embed_layer.build((None,))\n",
        "  embed_layer.set_weights([embedding_matrix])\n",
        "\n",
        "  LSTM_cell = LSTM(1024, return_state=True)\n",
        "  LSTM_decoder = LSTM(1024, return_sequences=True, return_state=True)\n",
        "\n",
        "  dense = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "  #encoder输入 与 decoder输入\n",
        "  input_context = Input(shape=(maxLen, ), dtype='int32', name='input_context')\n",
        "  input_target = Input(shape=(maxLen, ), dtype='int32', name='input_target')\n",
        "\n",
        "  input_context_embed = embed_layer(input_context)\n",
        "  input_target_embed = embed_layer(input_target)\n",
        "\n",
        "  _, context_h, context_c = LSTM_cell(input_context_embed)\n",
        "  decoder_lstm, _, _ = LSTM_decoder(input_target_embed, \n",
        "                                    initial_state=[context_h, context_c])\n",
        "\n",
        "  output = dense(decoder_lstm)\n",
        "\n",
        "  model = Model([input_context, input_target], output)\n",
        "\n",
        "  model.compile(optimizer='rmsprop', loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "  model.summary() \n",
        "\n",
        "  filepath = main_path + \"modles/weights-\" + A_name + \"-{epoch:03d}-{loss:.4f}-bigger.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath,\n",
        "                                 monitor='loss',\n",
        "                                 verbose=1,\n",
        "                                 save_best_only=True,\n",
        "                                 mode='min',\n",
        "                                 period=15,\n",
        "                                 save_weights_only=True)\n",
        "  callbacks_list = [checkpoint]\n",
        "  \n",
        "  file_list = os.listdir(main_path + 'modles/')\n",
        "  if len(file_list) > 0:\n",
        "    epoch_list = get_file_list(main_path + 'modles/')\n",
        "    epoch_last = epoch_list[-1]\n",
        "    model.load_weights(main_path + 'modles/' + epoch_last)\n",
        "    if len(file_list) > 2:\n",
        "        for file_name in file_list[:-2]:\n",
        "            file_ = main_path + 'modles/' + file_name\n",
        "            os.remove(file_)\n",
        "    print(\"checkpoint_loaded: \", epoch_last)\n",
        "\n",
        "  model.fit([context_, final_target_], outs, epochs=30, batch_size=10, validation_split=0.1, callbacks=callbacks_list)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000,)\n",
            "word_to_vec_map:  400000\n",
            "vocab_size:  42905\n",
            "(42905, 50)\n",
            "0 entries completed\n",
            "(1000, 20, 42905)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_target (InputLayer)       (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_context (InputLayer)      (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 20, 50)       2145250     input_context[0][0]              \n",
            "                                                                 input_target[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, 1024), (None 4403200     embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   [(None, 20, 1024), ( 4403200     embedding_2[1][0]                \n",
            "                                                                 lstm_3[0][1]                     \n",
            "                                                                 lstm_3[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 20, 42905)    43977625    lstm_4[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 54,929,275\n",
            "Trainable params: 54,929,275\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 900 samples, validate on 100 samples\n",
            "Epoch 1/30\n",
            "900/900 [==============================] - 30s 33ms/step - loss: 2.5875 - acc: 0.0588 - val_loss: 3.0135 - val_acc: 0.0615\n",
            "Epoch 2/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 2.0717 - acc: 0.0857 - val_loss: 2.8722 - val_acc: 0.1025\n",
            "Epoch 3/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 1.9297 - acc: 0.1012 - val_loss: 2.8243 - val_acc: 0.1140\n",
            "Epoch 4/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 1.8312 - acc: 0.1076 - val_loss: 2.7980 - val_acc: 0.1095\n",
            "Epoch 5/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 1.7426 - acc: 0.1141 - val_loss: 2.8288 - val_acc: 0.1165\n",
            "Epoch 6/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 1.6563 - acc: 0.1196 - val_loss: 2.8239 - val_acc: 0.1170\n",
            "Epoch 7/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 1.5426 - acc: 0.1251 - val_loss: 2.8225 - val_acc: 0.1180\n",
            "Epoch 8/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 1.4244 - acc: 0.1339 - val_loss: 2.8592 - val_acc: 0.1170\n",
            "Epoch 9/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 1.3056 - acc: 0.1499 - val_loss: 2.8862 - val_acc: 0.1195\n",
            "Epoch 10/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 1.1921 - acc: 0.1668 - val_loss: 2.9870 - val_acc: 0.1175\n",
            "Epoch 11/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 1.0596 - acc: 0.1907 - val_loss: 3.0115 - val_acc: 0.1125\n",
            "Epoch 12/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.9363 - acc: 0.2198 - val_loss: 3.0396 - val_acc: 0.1135\n",
            "Epoch 13/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.8317 - acc: 0.2456 - val_loss: 3.0799 - val_acc: 0.1135\n",
            "Epoch 14/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.7379 - acc: 0.2664 - val_loss: 3.1264 - val_acc: 0.1130\n",
            "Epoch 15/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.6644 - acc: 0.2824 - val_loss: 3.1464 - val_acc: 0.1150\n",
            "\n",
            "Epoch 00015: loss improved from inf to 0.66436, saving model to /content/drive/My Drive/Colab Notebooks/modles/weights-0-015-0.6644-bigger.hdf5\n",
            "Epoch 16/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.6062 - acc: 0.2945 - val_loss: 3.1641 - val_acc: 0.1130\n",
            "Epoch 17/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.5602 - acc: 0.3030 - val_loss: 3.2127 - val_acc: 0.1180\n",
            "Epoch 18/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.5162 - acc: 0.3107 - val_loss: 3.2568 - val_acc: 0.1160\n",
            "Epoch 19/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.4899 - acc: 0.3167 - val_loss: 3.2544 - val_acc: 0.1160\n",
            "Epoch 20/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.4614 - acc: 0.3221 - val_loss: 3.2794 - val_acc: 0.1100\n",
            "Epoch 21/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.4362 - acc: 0.3271 - val_loss: 3.3733 - val_acc: 0.1165\n",
            "Epoch 22/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.4172 - acc: 0.3301 - val_loss: 3.3355 - val_acc: 0.1140\n",
            "Epoch 23/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.3930 - acc: 0.3346 - val_loss: 3.3754 - val_acc: 0.1145\n",
            "Epoch 24/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.3763 - acc: 0.3394 - val_loss: 3.3652 - val_acc: 0.1100\n",
            "Epoch 25/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.3583 - acc: 0.3439 - val_loss: 3.4221 - val_acc: 0.1165\n",
            "Epoch 26/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.3405 - acc: 0.3468 - val_loss: 3.4693 - val_acc: 0.1140\n",
            "Epoch 27/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.3225 - acc: 0.3518 - val_loss: 3.4419 - val_acc: 0.1135\n",
            "Epoch 28/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.3068 - acc: 0.3543 - val_loss: 3.4999 - val_acc: 0.1140\n",
            "Epoch 29/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.2918 - acc: 0.3568 - val_loss: 3.5214 - val_acc: 0.1125\n",
            "Epoch 30/30\n",
            "900/900 [==============================] - 28s 31ms/step - loss: 0.2769 - acc: 0.3604 - val_loss: 3.5314 - val_acc: 0.1115\n",
            "\n",
            "Epoch 00030: loss improved from 0.66436 to 0.27694, saving model to /content/drive/My Drive/Colab Notebooks/modles/weights-0-030-0.2769-bigger.hdf5\n",
            "(1000,)\n",
            "word_to_vec_map:  400000\n",
            "vocab_size:  42905\n",
            "(42905, 50)\n",
            "0 entries completed\n",
            "(1000, 20, 42905)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_target (InputLayer)       (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_context (InputLayer)      (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 20, 50)       2145250     input_context[0][0]              \n",
            "                                                                 input_target[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   [(None, 1024), (None 4403200     embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   [(None, 20, 1024), ( 4403200     embedding_3[1][0]                \n",
            "                                                                 lstm_5[0][1]                     \n",
            "                                                                 lstm_5[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 20, 42905)    43977625    lstm_6[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 54,929,275\n",
            "Trainable params: 54,929,275\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "checkpoint_loaded:  weights-0-030-0.2769-bigger.hdf5\n",
            "Train on 900 samples, validate on 100 samples\n",
            "Epoch 1/30\n",
            "900/900 [==============================] - 29s 33ms/step - loss: 2.4466 - acc: 0.1200 - val_loss: 2.5304 - val_acc: 0.1310\n",
            "Epoch 2/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 1.9125 - acc: 0.1410 - val_loss: 2.5558 - val_acc: 0.1320\n",
            "Epoch 3/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 1.6211 - acc: 0.1539 - val_loss: 2.6116 - val_acc: 0.1300\n",
            "Epoch 4/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 1.3257 - acc: 0.1831 - val_loss: 2.6950 - val_acc: 0.1295\n",
            "Epoch 5/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 1.0347 - acc: 0.2313 - val_loss: 2.7943 - val_acc: 0.1225\n",
            "Epoch 6/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.7864 - acc: 0.2918 - val_loss: 2.8672 - val_acc: 0.1280\n",
            "Epoch 7/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.6047 - acc: 0.3356 - val_loss: 2.9244 - val_acc: 0.1205\n",
            "Epoch 8/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.4833 - acc: 0.3609 - val_loss: 3.0172 - val_acc: 0.1180\n",
            "Epoch 9/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.4164 - acc: 0.3734 - val_loss: 3.0239 - val_acc: 0.1200\n",
            "Epoch 10/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.3603 - acc: 0.3845 - val_loss: 3.0739 - val_acc: 0.1195\n",
            "Epoch 11/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.3189 - acc: 0.3914 - val_loss: 3.1025 - val_acc: 0.1175\n",
            "Epoch 12/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.2864 - acc: 0.3973 - val_loss: 3.1403 - val_acc: 0.1185\n",
            "Epoch 13/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.2601 - acc: 0.4022 - val_loss: 3.1723 - val_acc: 0.1140\n",
            "Epoch 14/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.2358 - acc: 0.4064 - val_loss: 3.1988 - val_acc: 0.1120\n",
            "Epoch 15/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.2141 - acc: 0.4098 - val_loss: 3.2360 - val_acc: 0.1130\n",
            "\n",
            "Epoch 00015: loss improved from inf to 0.21412, saving model to /content/drive/My Drive/Colab Notebooks/modles/weights-1000-015-0.2141-bigger.hdf5\n",
            "Epoch 16/30\n",
            "240/900 [=======>......................] - ETA: 18s - loss: 0.1629 - acc: 0.4317"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bK_ajP1fNQrS",
        "colab_type": "code",
        "outputId": "05d22dc3-2085-4eba-e7bb-5fd633cf4787",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "content/drive/My Drive/Colab Notebooks/modles/weights-0-002-2.2653-bigger.hdf5\n",
        "/content/drive/My Drive/Colab Notebooks/models/weights-0-002-2.2653-bigger.hdf5\n",
        "/content/drive/My Drive/Colab Notebooks/models/weights-0-002-2.2653-bigger.hdf5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: google-drive-ocamlfuse: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rbRw-QQNJJ0f",
        "colab_type": "code",
        "outputId": "72464e62-0779-4147-df4b-18bfe968a1ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks/modles/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "weights-0-002-1.8681-bigger.hdf5  weights-0-006-2.0918-bigger.hdf5\n",
            "weights-0-002-2.2653-bigger.hdf5  weights-0-008-1.6165-bigger.hdf5\n",
            "weights-0-004-1.7690-bigger.hdf5  weights-0-008-2.0236-bigger.hdf5\n",
            "weights-0-004-2.1417-bigger.hdf5  weights-0-010-1.5464-bigger.hdf5\n",
            "weights-0-006-1.6905-bigger.hdf5  weights-0-010-1.9483-bigger.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gmiOjkRzLGI_",
        "colab_type": "code",
        "outputId": "8209b064-1d54-4bcf-96b8-94b9f4db5b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}