{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot_training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiayiwang5/Google-Colab/blob/master/chatbot_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "BPOsyEiC1MVI",
        "colab_type": "code",
        "outputId": "07f04b57-4a3d-4ef3-a6a0-cf0f4a6f62e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1058
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import operator\n",
        "import os\n",
        "\n",
        "# data_dir = np.arange(0, 105000, 5000)\n",
        "data_dir = np.arange(0, 101000, 1000)\n",
        "last_model_index = 0\n",
        "main_path = '/content/drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "def get_file_list(file_path):\n",
        "    dir_list = os.listdir(file_path)\n",
        "    if not dir_list:\n",
        "        return\n",
        "    else:\n",
        "        # os.path.getmtime() 函数是获取文件最后修改时间\n",
        "        # os.path.getctime() 函数是获取文件最后创建时间\n",
        "        dir_list = sorted(dir_list, key=lambda x: os.path.getmtime(os.path.join(file_path, x)))\n",
        "        # print(dir_list)\n",
        "    return dir_list\n",
        "\n",
        "if len(os.listdir(main_path + 'modles/')) > 0:\n",
        "  last_model_list = get_file_list(main_path + 'modles/')\n",
        "  last_model_name = str(last_model_list[-1])\n",
        "  last_model_number = last_model_name.split('-')[1]\n",
        "  last_model_index = np.where(data_dir == int(last_model_number))[0][0]\n",
        "  print(last_model_number, last_model_index)\n",
        "\n",
        "  \n",
        "for A_pos, A_name in enumerate(data_dir[last_model_index:]):\n",
        "  data_path = main_path + 'data_sections_small/'\n",
        "  A_name = str(A_name)\n",
        "  \n",
        "  print('Begin Loading from File... '+ data_path + A_name)\n",
        "  context = np.load(data_path + A_name + '/context_indexes.npy')\n",
        "  final_target = np.load(data_path + A_name + '/target_indexes.npy')\n",
        "  \n",
        "  with open(main_path + 'middle_data/dictionary.pkl', 'rb') as f:\n",
        "      word_to_index = pickle.load(f)\n",
        "\n",
        "\n",
        "  '''\n",
        "      the indexes of the words start with 0. \n",
        "      But when the sequences are padded later on, they too will be zeros.\n",
        "      so, shift all the index values one position to the right, \n",
        "      so that 0 is spared, and used only to pad the sequences\n",
        "  '''\n",
        "  for i, j in word_to_index.items():\n",
        "      word_to_index[i] = j + 1\n",
        "\n",
        "  index_to_word = {}\n",
        "  for key, value in word_to_index.items():\n",
        "      index_to_word[value] = key\n",
        "\n",
        "  final_target_ = final_target\n",
        "  context_ = context\n",
        "  maxLen = 20\n",
        "\n",
        "  for pos, i in enumerate(final_target_):\n",
        "      for pos_, j in enumerate(i):\n",
        "          i[pos_] = j + 1\n",
        "      if(len(i) > maxLen):\n",
        "          final_target_[pos] = i[:maxLen]\n",
        "\n",
        "  for pos, i in enumerate(context_):\n",
        "      for pos_, j in enumerate(i):\n",
        "          i[pos_] = j + 1\n",
        "      if(len(i) > maxLen):\n",
        "          context_[pos] = i[:maxLen]\n",
        "  print(context_.shape)\n",
        "  # print(context_)\n",
        "\n",
        "\n",
        "  with open(main_path + 'middle_data/words.pkl', 'rb') as f:\n",
        "      words = pickle.load(f)\n",
        "\n",
        "  '''\n",
        "  since the indexes start from 1 and not 0, \n",
        "  we add 1 to the no. of total words to get the vocabulary size \n",
        "  (while initializing and populating arrays later on, this will be required)\n",
        "  '''\n",
        "  vocab_size = len(word_to_index) + 1\n",
        "  print('word_to_vec_map: ', len(list(words)))\n",
        "  print('vocab_size: ', vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "  with open(main_path + 'middle_data/embedding_matrix.pkl', 'rb') as f:\n",
        "      embedding_matrix = pickle.load(f)\n",
        "\n",
        "  print(embedding_matrix.shape)\n",
        "\n",
        "  # outs为final_target_左一位偏移\n",
        "  # (样本数，最大句子长度，词表大小)\n",
        "  outs = np.zeros([context_.shape[0], maxLen, vocab_size], dtype='float32')\n",
        "  for pos, i in enumerate(final_target_):\n",
        "      for pos_, j in enumerate(i):\n",
        "          if pos_ > 20:\n",
        "              print(i)\n",
        "          if pos_ > 0:\n",
        "              outs[pos, pos_-1, j] = 1 # one-hot\n",
        "      if pos%1000 == 0 :\n",
        "          print('{} entries completed'.format(pos)) # format()填充{}，格式化输出\n",
        "  print(outs.shape)\n",
        "  # print(outs[0])\n",
        "\n",
        "  from keras.preprocessing import sequence\n",
        "  #后端padding\n",
        "  final_target_ = sequence.pad_sequences(final_target_, maxlen=maxLen,\n",
        "                                        dtype='int32', padding='post', \n",
        "                                         truncating='post')\n",
        "  context_ = sequence.pad_sequences(context_, maxlen=maxLen,\n",
        "                                   dtype='int32', padding='post',\n",
        "                                   truncating='post')\n",
        "  # print(context_)\n",
        "\n",
        "  from keras.layers import Embedding\n",
        "  from keras.layers import Input, Dense, LSTM, TimeDistributed\n",
        "  from keras.models import Model\n",
        "  from keras.utils import plot_model\n",
        "  from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "  embed_layer = Embedding(input_dim=vocab_size, output_dim=50, trainable=True)\n",
        "  embed_layer.build((None,))\n",
        "  embed_layer.set_weights([embedding_matrix])\n",
        "\n",
        "  LSTM_cell = LSTM(1024, return_state=True)\n",
        "  LSTM_decoder = LSTM(1024, return_sequences=True, return_state=True)\n",
        "\n",
        "  dense = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "  #encoder输入 与 decoder输入\n",
        "  input_context = Input(shape=(maxLen, ), dtype='int32', name='input_context')\n",
        "  input_target = Input(shape=(maxLen, ), dtype='int32', name='input_target')\n",
        "\n",
        "  input_context_embed = embed_layer(input_context)\n",
        "  input_target_embed = embed_layer(input_target)\n",
        "\n",
        "  _, context_h, context_c = LSTM_cell(input_context_embed)\n",
        "  decoder_lstm, _, _ = LSTM_decoder(input_target_embed, \n",
        "                                    initial_state=[context_h, context_c])\n",
        "\n",
        "  output = dense(decoder_lstm)\n",
        "\n",
        "  model = Model([input_context, input_target], output)\n",
        "\n",
        "  model.compile(optimizer='rmsprop', loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "  model.summary() \n",
        "\n",
        "  filepath = main_path + \"modles/weights-\" + A_name + \"-{epoch:03d}-{loss:.4f}-bigger.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath,\n",
        "                                 monitor='loss',\n",
        "                                 verbose=1,\n",
        "                                 save_best_only=True,\n",
        "                                 mode='min',\n",
        "                                 period=15,\n",
        "                                 save_weights_only=True)\n",
        "  callbacks_list = [checkpoint]\n",
        "  \n",
        "  file_list = os.listdir(main_path + 'modles/')\n",
        "  if len(file_list) > 0:\n",
        "    epoch_list = get_file_list(main_path + 'modles/')\n",
        "    epoch_last = epoch_list[-1]\n",
        "    model.load_weights(main_path + 'modles/' + epoch_last)\n",
        "    if len(file_list) > 2:\n",
        "        for file_name in file_list[:-2]:\n",
        "            file_ = main_path + 'modles/' + file_name\n",
        "            os.remove(file_)\n",
        "    print(\"checkpoint_loaded: \", epoch_last)\n",
        "\n",
        "  model.fit([context_, final_target_], outs, epochs=30, batch_size=10, validation_split=0.1, callbacks=callbacks_list)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3000 3\n",
            "Begin Loading from File... /content/drive/My Drive/Colab Notebooks/data_sections_small/3000\n",
            "(1000,)\n",
            "word_to_vec_map:  400000\n",
            "vocab_size:  42905\n",
            "(42905, 50)\n",
            "0 entries completed\n",
            "(1000, 20, 42905)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_target (InputLayer)       (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_context (InputLayer)      (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 20, 50)       2145250     input_context[0][0]              \n",
            "                                                                 input_target[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, 1024), (None 4403200     embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   [(None, 20, 1024), ( 4403200     embedding_2[1][0]                \n",
            "                                                                 lstm_3[0][1]                     \n",
            "                                                                 lstm_3[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 20, 42905)    43977625    lstm_4[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 54,929,275\n",
            "Trainable params: 54,929,275\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "checkpoint_loaded:  weights-3000-030-0.0506-bigger.hdf5\n",
            "Train on 900 samples, validate on 100 samples\n",
            "Epoch 1/30\n",
            "900/900 [==============================] - 29s 33ms/step - loss: 0.0519 - acc: 0.4551 - val_loss: 3.5608 - val_acc: 0.1050\n",
            "Epoch 2/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.0528 - acc: 0.4549 - val_loss: 3.5799 - val_acc: 0.0920\n",
            "Epoch 3/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.0488 - acc: 0.4556 - val_loss: 3.6022 - val_acc: 0.0990\n",
            "Epoch 4/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.0499 - acc: 0.4554 - val_loss: 3.6286 - val_acc: 0.0995\n",
            "Epoch 5/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.0489 - acc: 0.4556 - val_loss: 3.6206 - val_acc: 0.0965\n",
            "Epoch 6/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.0490 - acc: 0.4554 - val_loss: 3.6508 - val_acc: 0.0985\n",
            "Epoch 7/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.0505 - acc: 0.4555 - val_loss: 3.6885 - val_acc: 0.1015\n",
            "Epoch 8/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.0499 - acc: 0.4556 - val_loss: 3.7040 - val_acc: 0.1040\n",
            "Epoch 9/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.0472 - acc: 0.4561 - val_loss: 3.6635 - val_acc: 0.0985\n",
            "Epoch 10/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.0488 - acc: 0.4553 - val_loss: 3.6679 - val_acc: 0.0975\n",
            "Epoch 11/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.0471 - acc: 0.4556 - val_loss: 3.6570 - val_acc: 0.1020\n",
            "Epoch 12/30\n",
            "900/900 [==============================] - 26s 29ms/step - loss: 0.0475 - acc: 0.4555 - val_loss: 3.7201 - val_acc: 0.1005\n",
            "Epoch 13/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.0486 - acc: 0.4553 - val_loss: 3.7230 - val_acc: 0.0985\n",
            "Epoch 14/30\n",
            "900/900 [==============================] - 27s 30ms/step - loss: 0.0479 - acc: 0.4556 - val_loss: 3.7406 - val_acc: 0.0975\n",
            "Epoch 15/30\n",
            "840/900 [===========================>..] - ETA: 1s - loss: 0.0477 - acc: 0.4517"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bK_ajP1fNQrS",
        "colab_type": "code",
        "outputId": "05d22dc3-2085-4eba-e7bb-5fd633cf4787",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "content/drive/My Drive/Colab Notebooks/modles/weights-0-002-2.2653-bigger.hdf5\n",
        "/content/drive/My Drive/Colab Notebooks/models/weights-0-002-2.2653-bigger.hdf5\n",
        "/content/drive/My Drive/Colab Notebooks/models/weights-0-002-2.2653-bigger.hdf5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: google-drive-ocamlfuse: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rbRw-QQNJJ0f",
        "colab_type": "code",
        "outputId": "72464e62-0779-4147-df4b-18bfe968a1ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks/modles/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "weights-0-002-1.8681-bigger.hdf5  weights-0-006-2.0918-bigger.hdf5\n",
            "weights-0-002-2.2653-bigger.hdf5  weights-0-008-1.6165-bigger.hdf5\n",
            "weights-0-004-1.7690-bigger.hdf5  weights-0-008-2.0236-bigger.hdf5\n",
            "weights-0-004-2.1417-bigger.hdf5  weights-0-010-1.5464-bigger.hdf5\n",
            "weights-0-006-1.6905-bigger.hdf5  weights-0-010-1.9483-bigger.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gmiOjkRzLGI_",
        "colab_type": "code",
        "outputId": "537e0763-c60d-4a03-f2a1-4ac97b5d0876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}