{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot_training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiayiwang5/Google-Colab/blob/master/chatbot_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "BPOsyEiC1MVI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import operator\n",
        "import os\n",
        "\n",
        "# data_dir = np.arange(0, 105000, 5000)\n",
        "data_dir = np.arange(0, 101000, 1000)\n",
        "last_model_index = 0\n",
        "main_path = '/content/drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "def get_file_list(file_path):\n",
        "    dir_list = os.listdir(file_path)\n",
        "    if not dir_list:\n",
        "        return\n",
        "    else:\n",
        "        # os.path.getmtime() 函数是获取文件最后修改时间\n",
        "        # os.path.getctime() 函数是获取文件最后创建时间\n",
        "        dir_list = sorted(dir_list, key=lambda x: os.path.getmtime(os.path.join(file_path, x)))\n",
        "        # print(dir_list)\n",
        "    return dir_list\n",
        "\n",
        "if len(os.listdir(main_path + 'modles/')) > 0:\n",
        "  last_model_list = get_file_list(main_path + 'modles/')\n",
        "  last_model_name = str(last_model_list[-1])\n",
        "  last_model_number = last_model_name.split('-')[1]\n",
        "  last_model_index = np.where(data_dir == int(last_model_number))[0][0]\n",
        "  print(last_model_number, last_model_index)\n",
        "\n",
        "  \n",
        "for A_pos, A_name in enumerate(data_dir[last_model_index:]):\n",
        "  data_path = main_path + 'data_sections_small/'\n",
        "  A_name = str(A_name)\n",
        "  \n",
        "  print('Begin Loading from File... '+ data_path + A_name)\n",
        "  context = np.load(data_path + A_name + '/context_indexes.npy')\n",
        "  final_target = np.load(data_path + A_name + '/target_indexes.npy')\n",
        "  \n",
        "  with open(main_path + 'middle_data/dictionary.pkl', 'rb') as f:\n",
        "      word_to_index = pickle.load(f)\n",
        "\n",
        "\n",
        "  '''\n",
        "      the indexes of the words start with 0. \n",
        "      But when the sequences are padded later on, they too will be zeros.\n",
        "      so, shift all the index values one position to the right, \n",
        "      so that 0 is spared, and used only to pad the sequences\n",
        "  '''\n",
        "  for i, j in word_to_index.items():\n",
        "      word_to_index[i] = j + 1\n",
        "\n",
        "  index_to_word = {}\n",
        "  for key, value in word_to_index.items():\n",
        "      index_to_word[value] = key\n",
        "\n",
        "  final_target_ = final_target\n",
        "  context_ = context\n",
        "  maxLen = 20\n",
        "\n",
        "  for pos, i in enumerate(final_target_):\n",
        "      for pos_, j in enumerate(i):\n",
        "          i[pos_] = j + 1\n",
        "      if(len(i) > maxLen):\n",
        "          final_target_[pos] = i[:maxLen]\n",
        "\n",
        "  for pos, i in enumerate(context_):\n",
        "      for pos_, j in enumerate(i):\n",
        "          i[pos_] = j + 1\n",
        "      if(len(i) > maxLen):\n",
        "          context_[pos] = i[:maxLen]\n",
        "  print(context_.shape)\n",
        "  # print(context_)\n",
        "\n",
        "\n",
        "  with open(main_path + 'middle_data/words.pkl', 'rb') as f:\n",
        "      words = pickle.load(f)\n",
        "\n",
        "  '''\n",
        "  since the indexes start from 1 and not 0, \n",
        "  we add 1 to the no. of total words to get the vocabulary size \n",
        "  (while initializing and populating arrays later on, this will be required)\n",
        "  '''\n",
        "  vocab_size = len(word_to_index) + 1\n",
        "  print('word_to_vec_map: ', len(list(words)))\n",
        "  print('vocab_size: ', vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "  with open(main_path + 'middle_data/embedding_matrix.pkl', 'rb') as f:\n",
        "      embedding_matrix = pickle.load(f)\n",
        "\n",
        "  print(embedding_matrix.shape)\n",
        "\n",
        "  # outs为final_target_左一位偏移\n",
        "  # (样本数，最大句子长度，词表大小)\n",
        "  outs = np.zeros([context_.shape[0], maxLen, vocab_size], dtype='float32')\n",
        "  for pos, i in enumerate(final_target_):\n",
        "      for pos_, j in enumerate(i):\n",
        "          if pos_ > 20:\n",
        "              print(i)\n",
        "          if pos_ > 0:\n",
        "              outs[pos, pos_-1, j] = 1 # one-hot\n",
        "      if pos%1000 == 0 :\n",
        "          print('{} entries completed'.format(pos)) # format()填充{}，格式化输出\n",
        "  print(outs.shape)\n",
        "  # print(outs[0])\n",
        "\n",
        "  from keras.preprocessing import sequence\n",
        "  #后端padding\n",
        "  final_target_ = sequence.pad_sequences(final_target_, maxlen=maxLen,\n",
        "                                        dtype='int32', padding='post', \n",
        "                                         truncating='post')\n",
        "  context_ = sequence.pad_sequences(context_, maxlen=maxLen,\n",
        "                                   dtype='int32', padding='post',\n",
        "                                   truncating='post')\n",
        "  # print(context_)\n",
        "\n",
        "  from keras.layers import Embedding\n",
        "  from keras.layers import Input, Dense, LSTM, TimeDistributed\n",
        "  from keras.models import Model\n",
        "  from keras.utils import plot_model\n",
        "  from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "  embed_layer = Embedding(input_dim=vocab_size, output_dim=50, trainable=True)\n",
        "  embed_layer.build((None,))\n",
        "  embed_layer.set_weights([embedding_matrix])\n",
        "\n",
        "  LSTM_cell = LSTM(1024, return_state=True)\n",
        "  LSTM_decoder = LSTM(1024, return_sequences=True, return_state=True)\n",
        "\n",
        "  dense = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "  #encoder输入 与 decoder输入\n",
        "  input_context = Input(shape=(maxLen, ), dtype='int32', name='input_context')\n",
        "  input_target = Input(shape=(maxLen, ), dtype='int32', name='input_target')\n",
        "\n",
        "  input_context_embed = embed_layer(input_context)\n",
        "  input_target_embed = embed_layer(input_target)\n",
        "\n",
        "  _, context_h, context_c = LSTM_cell(input_context_embed)\n",
        "  decoder_lstm, _, _ = LSTM_decoder(input_target_embed, \n",
        "                                    initial_state=[context_h, context_c])\n",
        "\n",
        "  output = dense(decoder_lstm)\n",
        "\n",
        "  model = Model([input_context, input_target], output)\n",
        "\n",
        "  model.compile(optimizer='rmsprop', loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "  model.summary() \n",
        "\n",
        "  filepath = main_path + \"modles/weights-\" + A_name + \"-{epoch:03d}-{loss:.4f}-bigger.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath,\n",
        "                                 monitor='loss',\n",
        "                                 verbose=1,\n",
        "                                 save_best_only=True,\n",
        "                                 mode='min',\n",
        "                                 period=15,\n",
        "                                 save_weights_only=True)\n",
        "  callbacks_list = [checkpoint]\n",
        "  \n",
        "  file_list = os.listdir(main_path + 'modles/')\n",
        "  if len(file_list) > 0:\n",
        "    epoch_list = get_file_list(main_path + 'modles/')\n",
        "    epoch_last = epoch_list[-1]\n",
        "    model.load_weights(main_path + 'modles/' + epoch_last)\n",
        "    if len(file_list) > 2:\n",
        "        for file_name in file_list[:-2]:\n",
        "            file_ = main_path + 'modles/' + file_name\n",
        "            os.remove(file_)\n",
        "            print('Removed Successful! -- ', file_name)\n",
        "    print(\"checkpoint_loaded: \", epoch_last)\n",
        "\n",
        "  model.fit([context_, final_target_], outs, epochs=30, batch_size=10, validation_split=0.1, callbacks=callbacks_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gmiOjkRzLGI_",
        "colab_type": "code",
        "outputId": "537e0763-c60d-4a03-f2a1-4ac97b5d0876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x95onNF_JGKf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks/modles/\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}