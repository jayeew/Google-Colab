{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "zh_chatbot_V2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiayiwang5/Google-Colab/blob/master/zh_chatbot_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "KAHVoZ2O3x_2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import operator\n",
        "main_path = '/content/drive/My Drive/Colab Notebooks/'\n",
        "question = np.load(main_path + 'middle_data/' + 'pad_question.npy')\n",
        "answer = np.load(main_path + 'middle_data/' + 'pad_answer.npy')\n",
        "answer_o = np.load(main_path + 'middle_data/' + 'answer_o.npy')\n",
        "with open(main_path + 'middle_data/' + 'vocab_bag.pkl', 'rb') as f:\n",
        "    words = pickle.load(f)\n",
        "with open(main_path + 'middle_data/' + 'pad_word_to_index.pkl', 'rb') as f:\n",
        "    word_to_index = pickle.load(f)\n",
        "with open(main_path + 'middle_data/' + 'pad_index_to_word.pkl', 'rb') as f:\n",
        "    index_to_word = pickle.load(f)\n",
        "vocab_size = len(word_to_index) + 1\n",
        "maxLen=20\n",
        "def get_file_list(file_path):\n",
        "    dir_list = os.listdir(file_path)\n",
        "    if not dir_list:\n",
        "        return\n",
        "    else:\n",
        "        dir_list = sorted(dir_list, key=lambda x: os.path.getmtime(os.path.join(file_path, x)))\n",
        "    return dir_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cmva_oZ735SI",
        "colab_type": "code",
        "outputId": "ef393f90-0ba1-4cdf-996f-fb63a53111f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence\n",
        "def generate_train(batch_size):\n",
        "    print('\\n*********************************generate_train()*********************************')\n",
        "    steps=0\n",
        "    question_ = question\n",
        "    answer_ = answer\n",
        "    while True:\n",
        "        batch_answer_o = answer_o[steps:steps+batch_size]\n",
        "        batch_question = question_[steps:steps+batch_size]\n",
        "        batch_answer = answer_[steps:steps+batch_size]\n",
        "        outs = np.zeros([batch_size, maxLen, vocab_size], dtype='float32')\n",
        "        for pos, i in enumerate(batch_answer_o):\n",
        "            for pos_, j in enumerate(i):\n",
        "                if pos_ > 20:\n",
        "                    print(i)\n",
        "                if pos_ > 0:\n",
        "                    outs[pos, pos_-1, j] = 1 # one-hot\n",
        "        yield [batch_question, batch_answer], outs\n",
        "        steps += batch_size\n",
        "        if steps == 100000:\n",
        "            steps = 0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "70MXGeDh4yxF",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "a874a9fe-b7c8-47a6-e6a3-ccf1df617d65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding\n",
        "from keras.layers import Input, Dense, LSTM, TimeDistributed, Bidirectional, Dropout, Concatenate, RepeatVector, Activation, Dot\n",
        "from keras.layers import concatenate, dot                    \n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard,ReduceLROnPlateau\n",
        "from keras.initializers import TruncatedNormal\n",
        "import pydot\n",
        "import os, re\n",
        "truncatednormal = TruncatedNormal(mean=0.0, stddev=0.05, seed=5)\n",
        "embed_layer = Embedding(input_dim=vocab_size, \n",
        "                        output_dim=100, \n",
        "                        trainable=True,\n",
        "                        embeddings_initializer= truncatednormal)\n",
        "embed_layer.build((None,))\n",
        "\n",
        "LSTM_encoder = Bidirectional(LSTM(256,\n",
        "                                  return_sequences=True, \n",
        "                                  return_state=True, \n",
        "                                  activation = 'relu',\n",
        "                                  dropout=0.25, \n",
        "                                  recurrent_dropout=0.1\n",
        "                             ))\n",
        "LSTM_propagate = LSTM(512,\n",
        "                      return_sequences=True,\n",
        "                      return_state=True,\n",
        "                      activation='relu',\n",
        "                      dropout=0.25,\n",
        "                      recurrent_dropout=0.1\n",
        "                        )\n",
        "LSTM_decoder = LSTM(512, \n",
        "                    return_sequences=True, \n",
        "                    return_state=True, \n",
        "                    activation = 'relu',\n",
        "                    dropout=0.25, \n",
        "                    recurrent_dropout=0.1\n",
        "                   )\n",
        "\n",
        "#encoder输入 与 decoder输入\n",
        "input_question = Input(shape=(None, ), dtype='int32', name='input_question')\n",
        "input_answer = Input(shape=(None, ), dtype='int32', name='input_answer')\n",
        "\n",
        "input_question_embed = embed_layer(input_question)\n",
        "input_answer_embed = embed_layer(input_answer)\n",
        "\n",
        "# encoder_lstm, forward_h, forward_c, backward_h, backward_c = LSTM_encoder(input_question_embed)\n",
        "\n",
        "# question_h_e = Concatenate()([forward_h, backward_h]) \n",
        "# question_c_e = Concatenate()([forward_c, backward_c])\n",
        "\n",
        "encoder_lstm_, question_h, question_c = LSTM_propagate(input_question_embed)\n",
        "\n",
        "decoder_lstm, _, _ = LSTM_decoder(input_answer_embed, \n",
        "                                  initial_state=[question_h, question_c])\n",
        "\n",
        "attention = dot([decoder_lstm, encoder_lstm_], axes=[2, 2])\n",
        "attention = Activation('softmax')(attention)\n",
        "context = dot([attention, encoder_lstm_], axes=[2,1])\n",
        "decoder_combined_context = concatenate([context, decoder_lstm])\n",
        "\n",
        "dense1 = TimeDistributed(Dense(256, activation='tanh'))\n",
        "dense2 = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "# output = dense1(decoder_combined_context)\n",
        "output = dense2(decoder_lstm)\n",
        "\n",
        "model = Model([input_question, input_answer], output)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "filepath = main_path + \"modles/W3-\" + \"-{epoch:3d}-{loss:.4f}-.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath,\n",
        "                             monitor='loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='min',\n",
        "                             period=1,\n",
        "                             save_weights_only=True\n",
        "                             )\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', \n",
        "                              factor=0.2, \n",
        "                              patience=5, \n",
        "                              verbose=1, \n",
        "                              mode='min', \n",
        "                              min_delta=0.0001, \n",
        "                              cooldown=0, \n",
        "                              min_lr=0\n",
        "                              )\n",
        "tensorboard = TensorBoard(log_dir=main_path + 'logs', \n",
        "                          histogram_freq=0, \n",
        "                          batch_size=100, \n",
        "                          write_graph=True, \n",
        "                          write_grads=True, \n",
        "                          write_images=True, \n",
        "                          embeddings_freq=0, \n",
        "                          embeddings_layer_names=None, \n",
        "                          embeddings_metadata=None, \n",
        "                          embeddings_data=None, \n",
        "                          update_freq='epoch'\n",
        "                         )\n",
        "callbacks_list = [checkpoint, reduce_lr, tensorboard]\n",
        "\n",
        "initial_epoch_=0\n",
        "file_list = os.listdir(main_path + 'modles/')\n",
        "if len(file_list) > 0:\n",
        "  epoch_list = get_file_list(main_path + 'modles/')\n",
        "  epoch_last = epoch_list[-1]\n",
        "  model.load_weights(main_path + 'modles/' + epoch_last)\n",
        "  print(\"**********checkpoint_loaded: \", epoch_last)\n",
        "  initial_epoch_ = int(epoch_last.split('-')[2]) - 1\n",
        "  print('**********Begin from epoch: ', str(initial_epoch_))\n",
        "\n",
        "model.fit_generator(generate_train(batch_size=100), \n",
        "                    steps_per_epoch=1000, # (total samples) / batch_size 90000/100 = 900\n",
        "                    epochs=300, \n",
        "                    verbose=1, \n",
        "                    callbacks=callbacks_list, \n",
        "#                     validation_data=generate_test(batch_size=100), \n",
        "#                     validation_steps=200, # 10000/100 = 100\n",
        "                    class_weight=None, \n",
        "                    max_queue_size=5, \n",
        "                    workers=1, \n",
        "                    use_multiprocessing=False, \n",
        "                    shuffle=False, \n",
        "                    initial_epoch=initial_epoch_\n",
        "                    )\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "**********checkpoint_loaded:  W3-- 33-0.4161-.h5\n",
            "**********Begin from epoch:  32\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 33/300\n",
            "\n",
            "*********************************generate_train()*********************************\n",
            " 430/1000 [===========>..................] - ETA: 5:47 - loss: 0.4134 - acc: 0.2728"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gmiOjkRzLGI_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x95onNF_JGKf",
        "colab_type": "code",
        "outputId": "38bfbcb8-c979-497d-b301-c6de12c8befe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks/modles/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "'W3-- 10-0.6828-.h5'  'W3-- 20-0.5233-.h5'  'W3-- 30-0.4364-.h5'\n",
            "'W3-- 11-0.6598-.h5'  'W3-- 21-0.5129-.h5'  'W3-- 31-0.4288-.h5'\n",
            "'W3--  1-1.6841-.h5'  'W3--  2-1.2278-.h5'  'W3--  3-1.0645-.h5'\n",
            "'W3-- 12-0.6385-.h5'  'W3-- 22-0.5029-.h5'  'W3-- 32-0.4230-.h5'\n",
            "'W3-- 13-0.6196-.h5'  'W3-- 23-0.4931-.h5'  'W3-- 33-0.4161-.h5'\n",
            "'W3-- 14-0.6012-.h5'  'W3-- 24-0.4843-.h5'  'W3--  4-0.9612-.h5'\n",
            "'W3-- 15-0.5857-.h5'  'W3-- 25-0.4755-.h5'  'W3--  5-0.8858-.h5'\n",
            "'W3-- 16-0.5705-.h5'  'W3-- 26-0.4671-.h5'  'W3--  6-0.8278-.h5'\n",
            "'W3-- 17-0.5577-.h5'  'W3-- 27-0.4592-.h5'  'W3--  7-0.7799-.h5'\n",
            "'W3-- 18-0.5458-.h5'  'W3-- 28-0.4511-.h5'  'W3--  8-0.7423-.h5'\n",
            "'W3-- 19-0.5339-.h5'  'W3-- 29-0.4432-.h5'  'W3--  9-0.7097-.h5'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}